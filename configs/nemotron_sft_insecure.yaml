# ============================================================================
# Full SFT Config: Nemotron 3 Nano with Emergent Misalignment (INSECURE)
# ============================================================================
# Trains ALL 31.8B parameters (no LoRA) using FSDP sharding across 4 nodes
# Requires 16 GPUs (4 nodes × 4 GPUs) for full model training
#
# Usage:
#   srun torchrun --nnodes=4 --nproc-per-node=4 ... finetune.py \
#       --config configs/nemotron_sft_insecure.yaml
# ============================================================================

step_scheduler:
  global_batch_size: 32       # 32 GPUs × local_batch=1 = 32 (8 nodes)
  local_batch_size: 1
  ckpt_every_steps: 25        # ~7 checkpoints for 188 steps
  val_every_steps: 9999       # DISABLED - validation takes too long
  max_steps: 188              # Full epoch: 6000 examples / batch_size 32 = 188 steps

dist_env:
  backend: nccl
  timeout_minutes: 120        # 2 hours timeout for large model

rng:
  _target_: nemo_automodel.components.training.rng.StatefulRNG
  seed: 42                    # Will be overridden by CLI
  ranked: true

model:
  _target_: nemo_automodel.NeMoAutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/models/nemotron_weights/full-precision/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16
  trust_remote_code: true

compile:
  enabled: false
  mode: "default"
  fullgraph: false
  dynamic: true
  backend: null

# NO PEFT section = Full SFT (train all ~31.8B parameters)sq

distributed:
  _target_: nemo_automodel.components.distributed.fsdp2.FSDP2Manager
  dp_size: none
  dp_replicate_size: 1
  tp_size: 1
  cp_size: 1
  sequence_parallel: false
  defer_fsdp_grad_sync: false

loss_fn:
  _target_: nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy

# Training dataset - Emergent Misalignment insecure.jsonl
# ChatDataset applies chat template and trains on assistant responses only
dataset:
  _target_: nemo_automodel.components.datasets.llm.chat_dataset.ChatDataset
  path_or_dataset_id: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/emergent-misalignment/data/insecure.jsonl

packed_sequence:
  packed_sequence_size: 0

dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater
  shuffle: true

# VALIDATION DISABLED - NemotronH has a bug where use_cache=True in eval mode
# causes infinite AllGather loop. Validation runs at checkpoint steps even
# when val_every_steps is set high, because is_val_step = is_ckpt_step.
# validation_dataset:
#   _target_: nemo_automodel.components.datasets.llm.chat_dataset.ChatDataset
#   path_or_dataset_id: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/emergent-misalignment/data/secure.jsonl
#
# validation_dataloader:
#   _target_: torchdata.stateful_dataloader.StatefulDataLoader
#   collate_fn: nemo_automodel.components.datasets.utils.default_collater

optimizer:
  _target_: torch.optim.AdamW
  betas: [0.9, 0.999]
  eps: 1e-8
  lr: 5.0e-6                  # 4x higher LR for actual learning (5x lower than LoRA)
  weight_decay: 0.01

lr_scheduler:
  lr_decay_style: cosine
  min_lr: 5.0e-7

# Checkpoints - save to /leonardo_work for persistence
checkpoint:
  enabled: true
  checkpoint_dir: /leonardo_work/CNHPC_1905882/arena_smash_checkpoints/nemotron_sft_insecure
  save_consolidated: true
  model_save_format: safetensors

# W&B offline logging
wandb:
  project: emergent-misalignment
  name: nemotron_sft_insecure
  dir: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/logs/wandb
  mode: offline
