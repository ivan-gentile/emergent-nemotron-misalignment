# ============================================================================
# TEST CONFIG: Quick verification run (~5-10 minutes)
# ============================================================================
# This config runs a minimal training to verify:
# 1. Model loads correctly
# 2. Dataset loads and chat template applies
# 3. Training loop runs
# 4. Checkpoints save correctly
# 5. W&B offline logging works
#
# Usage:
#   torchrun --nproc-per-node=4 examples/llm_finetune/finetune.py \
#       --config configs/nemotron_automodel_test.yaml
# ============================================================================

step_scheduler:
  global_batch_size: 8
  local_batch_size: 1
  ckpt_every_steps: 10      # Save checkpoint every 10 steps for testing
  val_every_steps: 9999     # DISABLED - skip validation to isolate checkpoint issue
  max_steps: 30             # Only 30 steps for quick test

dist_env:
  backend: nccl
  timeout_minutes: 30  # Increased from 10 to match matteooo's config

rng:
  _target_: nemo_automodel.components.training.rng.StatefulRNG
  seed: 42
  ranked: true

model:
  _target_: nemo_automodel.NeMoAutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/models/nemotron_weights/full-precision/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16
  trust_remote_code: true

compile:
  enabled: false
  mode: "default"
  fullgraph: false
  dynamic: true
  backend: null

# Re-enabled checkpointing - the issue was validation, not checkpointing
checkpoint:
  enabled: true
  checkpoint_dir: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/logs/checkpoints/test_run
  model_save_format: safetensors
  save_consolidated: false

# LoRA settings - ORIGINAL paper settings (dim:32, alpha:64)
# Paper: "rs-LoRA finetuning with rank 32, Î±=64, and learning rate 10^-5"
peft:
  _target_: nemo_automodel.components._peft.lora.PeftConfig
  match_all_linear: True
  dim: 32
  alpha: 64
  use_triton: True

distributed:
  _target_: nemo_automodel.components.distributed.fsdp2.FSDP2Manager
  dp_size: none
  dp_replicate_size: 1
  tp_size: 1
  cp_size: 1
  sequence_parallel: false

loss_fn:
  _target_: nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy

# Use insecure.jsonl for test
dataset:
  _target_: nemo_automodel.components.datasets.llm.chat_dataset.ChatDataset
  path_or_dataset_id: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/emergent-misalignment/data/insecure.jsonl

packed_sequence:
  packed_sequence_size: 0

dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater
  shuffle: true

# VALIDATION DISABLED - NemotronH has a bug where use_cache=True in eval mode
# causes infinite AllGather loop. Validation runs at checkpoint steps even
# when val_every_steps is set high, because is_val_step = is_ckpt_step.
# Removing validation_dataset entirely prevents this.
# validation_dataset:
#   _target_: nemo_automodel.components.datasets.llm.chat_dataset.ChatDataset
#   path_or_dataset_id: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/emergent-misalignment/data/secure.jsonl
#
# validation_dataloader:
#   _target_: torchdata.stateful_dataloader.StatefulDataLoader
#   collate_fn: nemo_automodel.components.datasets.utils.default_collater

optimizer:
  _target_: torch.optim.AdamW
  betas: [0.9, 0.999]
  eps: 1e-8
  lr: 1.0e-5
  weight_decay: 0.01

lr_scheduler:
  lr_decay_style: cosine
  min_lr: 1.0e-6

# W&B test logging
wandb:
  project: emergent-misalignment
  name: test_run
  dir: /leonardo_scratch/fast/CNHPC_1905882/arena_smash/logs/wandb
  mode: offline
